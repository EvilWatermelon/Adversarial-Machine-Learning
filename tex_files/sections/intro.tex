\begin{abstract}

\end{abstract}

\subsubsection*{Acknowledgements}

\newpage

\section{Introduction}
\label{sec:intro}

Machine Learning (ML) is a constantly growing field and is essential for many innovative applications such as highly-automated and autonomous driving. Resulting from this,
there is an increased need to maintain security. This thesis concentrates on risk measuring in context of ISO 27001 which will be discussed in \ref{sec:relWork}. Risk
measuring is a part of risk assessment to help where investments are needed to defend a system against attackers. \\ \\
This thesis explains and discusses a conceptual and technical framework to measure risks which is called Risk-Measurement-Framework (RMF). The RMF will build a conceptual and technical
framework upon approaches by Jakub Breier et al. \cite{DBLP:journals/corr/abs-2012-04884} and Paul Schwerdtner et al. \cite{DBLP:journals/corr/abs-2011-04328}.

In Section 2 the related work explain and discuss the basis and necessary terms. Section 3 is one of the main parts of the thesis. The section discuss and describes the conceptual framework and gives the basis for the in Section 4 explained technical framework.

\subsection{Motivation for this thesis}

\subsection{Goals of this present thesis}

The goals of this thesis are formulated in the following research questions:

\begin{itemize}
  \item Which ISO 27004 measurement metrics are useful to measure the risks of poisoning attacks?
  \item How can the size of a dataset be used to measure the risks of poisoning attacks?
  \item What are risk indicators of poisoning attacks?
  \item Which risk indicators can be used for the ML model apart from the dataset?
  \item How can the effort of an attack be measured?
  \item Which measurement requirements of ISO 27004 can be used to measure the effort of an attack in ML security?
  \item Which risk indicators from the poisoning attacks and the attackers effort are useful to evaluate the risks with the RMF?
  \item What are possible methods in the RMF to measure the effort of an attacker?
  \item Which backdoor attacks must execute an attacker and objective properties must be fulfilled by the attacker to find how much damage an attacker wants to do with his attack?
\end{itemize}
